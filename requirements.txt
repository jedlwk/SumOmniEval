# SumOmniEval v2.0 - Requirements
# 24 metrics across 2 evaluation stages
# All local models < 2GB each | Total storage ~6GB

# =============================================================================
# Core Framework
# =============================================================================
streamlit==1.40.0           # Web UI
pandas==2.2.0               # Data handling
numpy==1.26.4               # CRITICAL: Must be 1.x for pyemd (MoverScore)
openpyxl==3.1.2             # Excel file support

# =============================================================================
# Deep Learning Stack
# =============================================================================
# PyTorch: Install separately for CPU-only: pip install torch --index-url https://download.pytorch.org/whl/cpu
transformers==4.45.0        # HuggingFace models (NLI, FactCC, AlignScore, Perplexity)
tokenizers==0.20.3          # Fast tokenization
sentencepiece==0.2.0        # Tokenizer for some models
protobuf==3.20.3            # Model serialization

# =============================================================================
# Stage 2 - Lexical Metrics (ROUGE, BLEU, METEOR, etc.)
# Models: None (rule-based) | Size: ~100MB total
# =============================================================================
rouge-score==0.1.2          # ROUGE-1/2/L
sacrebleu==2.4.0            # BLEU, chrF++
nltk==3.9.1                 # METEOR (requires WordNet data)
python-Levenshtein==0.25.0  # Levenshtein distance
Levenshtein==0.25.0         # Alternative binding

# =============================================================================
# Stage 2 - Semantic Metrics (BERTScore, MoverScore)
# Models: roberta-large (~1.4GB), distilbert (~260MB)
# =============================================================================
bert-score==0.3.13          # BERTScore (Precision/Recall/F1)
sentence-transformers==2.2.2 # Semantic Coverage (MiniLM ~80MB)
# MoverScore: pip install moverscore (or install from GitHub)

# =============================================================================
# Stage 1 - Faithfulness Metrics (NLI, FactCC, AlignScore)
# Models: deberta-v3-base (~440MB), deberta-base-mnli (~440MB), AlignScore (~1.4GB)
# =============================================================================
# Uses transformers (above) - models download on first use

# =============================================================================
# Stage 1 - Completeness (Coverage Score)
# Model: en_core_web_sm (~12MB)
# =============================================================================
spacy==3.7.5                # Named Entity Recognition
# Run after install: python -m spacy download en_core_web_sm

# =============================================================================
# Stage 1 - API Metrics (G-Eval, DAG, Prometheus)
# No local models - uses H2OGPTE API
# =============================================================================
h2ogpte==1.6.54             # H2OGPTE API client
python-dotenv==1.0.0        # Load .env credentials

# =============================================================================
# HuggingFace Ecosystem
# =============================================================================
huggingface-hub==0.25.2     # Model downloads
datasets==3.0.0             # Dataset utilities
evaluate==0.4.3             # Evaluation utilities

# =============================================================================
# Utilities
# =============================================================================
tqdm==4.66.5                # Progress bars
scikit-learn==1.5.2         # ML utilities
pyyaml==6.0.2               # Config parsing
regex==2024.9.11            # Advanced regex
safetensors==0.4.5          # Safe model loading
filelock==3.16.1            # File locking

# =============================================================================
# NOT INCLUDED (and why)
# =============================================================================
# bleurt - TensorFlow/PyTorch conflicts
# questeval - Cython dependency issues, abandoned (2022)
# unieval - Fallback unreliable, use G-Eval instead
